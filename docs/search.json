{
  "articles": [
    {
      "path": "cleansing.html",
      "title": "Cleansing the dataset",
      "description": "Cleansing the dataset for data analytics\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-30",
      "contents": "\n\nContents\nPreface\nData: create example dataset\nClean dataset\nCleanse dataset with cleanse()\n\nDiagnosis and removal of highly correlated variables\nCleanse dataset with treatment_corr()\n\n\n\n\n\nPreface\nIf you created a dataset to create a classification model, you must perform cleansing of the data. After you create the dataset, you should do the following:\nCleansing the dataset\nOptional removal of variables including missing values\nRemove a variable with one unique number\nRemove categorical variables with a large number of levels\nConvert a character variable to a categorical variable\n\nSplit the data into a train set and a test set\nModeling and Evaluate, Predict\nThe alookr package makes these steps fast and easy:\nData: create example dataset\nTo illustrate basic use of the alookr package, create the data_exam with sample function. The data_exam dataset include 5 variables.\nvariables are as follows.:\nid : character\nyear: character\ncount: numeric\nalpha : character\nflag : character\n\n\n# create sample dataset\nset.seed(123L)\nid <- sapply(1:1000, function(x)\n  paste(c(sample(letters, 5), x), collapse = \"\"))\n\nyear <- \"2018\"\n\nset.seed(123L)\ncount <- sample(1:10, size = 1000, replace = TRUE)\n\nset.seed(123L)\nalpha <- sample(letters, size = 1000, replace = TRUE)\n\nset.seed(123L)\nflag <- sample(c(\"Y\", \"N\"), size = 1000, prob = c(0.1, 0.9), replace = TRUE)\n\ndata_exam <- data.frame(id, year, count, alpha, flag, stringsAsFactors = FALSE)\n\n# structure of dataset\nstr(data_exam)\n\n\n'data.frame':   1000 obs. of  5 variables:\n $ id   : chr  \"osncj1\" \"rvket2\" \"nvesi3\" \"chgji4\" ...\n $ year : chr  \"2018\" \"2018\" \"2018\" \"2018\" ...\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: chr  \"o\" \"s\" \"n\" \"c\" ...\n $ flag : chr  \"N\" \"N\" \"N\" \"N\" ...\n\n\n# summary of dataset\nsummary(data_exam)\n\n\n      id                year               count       \n Length:1000        Length:1000        Min.   : 1.000  \n Class :character   Class :character   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median : 6.000  \n                                       Mean   : 5.698  \n                                       3rd Qu.: 8.000  \n                                       Max.   :10.000  \n    alpha               flag          \n Length:1000        Length:1000       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\nClean dataset\ncleanse() cleans up the dataset before fitting the classification model.\nThe function of cleanse() is as follows.:\nremove variables whose unique value is one\nremove variables with high unique rate\nconverts character variables to factor\nremove variables with missing value\nCleanse dataset with cleanse()\nFor example, we can cleanse all variables in data_exam:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n• flag\n\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nremove variables whose unique value is one : The year variable has only one value, “2018”. Not needed when fitting the model. So it was removed.\nremove variables with high unique rate : If the number of levels of categorical data is very large, it is not suitable for classification model. In this case, it is highly likely to be an identifier of the data. So, remove the categorical (or character) variable with a high value of the unique rate defined as “number of levels / number of observations”.\nThe unique rate of the id variable with the number of levels of 1000 is 1. This variable is the object of the removal by identifier.\nThe unique rate of the alpha variable is 0.026 and this variable is also removed.\n\nconverts character variables to factor : The character type flag variable is converted to a factor type.\nFor example, we can not remove the categorical data that is removed by changing the threshold of the unique rate:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam, uniq_thres = 0.03)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n• flag\n\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nThe alpha variable was not removed.\nIf you do not want to apply a unique rate, you can set the value of the uniq argument to FALSE.:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam, uniq = FALSE)\n\n\n── Checking character variables ─────────────────────── categorical data ──\n• id\n• year\n• alpha\n• flag\n\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  5 variables:\n $ id   : Factor w/ 1000 levels \"ablnc282\",\"abqym54\",..: 594 715 558 94 727 270 499 882 930 515 ...\n $ year : Factor w/ 1 level \"2018\": 1 1 1 1 1 1 1 1 1 1 ...\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nIf you do not want to force type conversion of a character variable to factor, you can set the value of the char argument to FALSE.:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam, char = FALSE)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: chr  \"o\" \"s\" \"n\" \"c\" ...\n $ flag : chr  \"N\" \"N\" \"N\" \"N\" ...\n\nIf you want to remove a variable that contains missing values, specify the value of the missing argument as TRUE. The following example removes the flag variable that contains the missing value.\n\n\ndata_exam$flag[1] <- NA \n\n# cleansing dataset\nnewDat <- cleanse(data_exam, missing = TRUE)\n\n\n── Checking missing value ────────────────────────────────── included NA ──\n• flag\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  2 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n\nDiagnosis and removal of highly correlated variables\nIn the linear model, there is a multicollinearity if there is a strong correlation between independent variables. So it is better to remove one variable from a pair of variables where the correlation exists.\nEven if it is not a linear model, removing one variable from a strongly correlated pair of variables can also reduce the overhead of the operation. It is also easy to interpret the model.\nCleanse dataset with treatment_corr()\ntreatment_corr() diagnose pairs of highly correlated variables or remove on of them.\ntreatment_corr() calculates correlation coefficient of pearson for numerical variable, and correlation coefficient of spearman for categorical variable.\nFor example, we can diagnosis and removal of highly correlated variables:\n\n\n# numerical variable\nx1 <- 1:100\nset.seed(12L)\nx2 <- sample(1:3, size = 100, replace = TRUE) * x1 + rnorm(1)\nset.seed(1234L)\nx3 <- sample(1:2, size = 100, replace = TRUE) * x1 + rnorm(1)\n\n# categorical variable\nx4 <- factor(rep(letters[1:20], time = 5))\nset.seed(100L)\nx5 <- factor(rep(letters[1:20 + sample(1:6, size = 20, replace = TRUE)], time = 5))\nset.seed(200L)\nx6 <- factor(rep(letters[1:20 + sample(1:3, size = 20, replace = TRUE)], time = 5))\nset.seed(300L)\nx7 <- factor(sample(letters[1:5], size = 100, replace = TRUE))\n\nexam <- data.frame(x1, x2, x3, x4, x5, x6, x7)\nstr(exam)\n\n\n'data.frame':   100 obs. of  7 variables:\n $ x1: int  1 2 3 4 5 6 7 8 9 10 ...\n $ x2: num  2.55 4.55 9.55 12.55 10.55 ...\n $ x3: num  0.194 2.194 4.194 6.194 3.194 ...\n $ x4: Factor w/ 20 levels \"a\",\"b\",\"c\",\"d\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ x5: Factor w/ 13 levels \"c\",\"e\",\"f\",\"g\",..: 1 5 3 2 4 7 6 8 9 8 ...\n $ x6: Factor w/ 15 levels \"c\",\"d\",\"f\",\"g\",..: 1 2 3 4 3 5 6 7 8 9 ...\n $ x7: Factor w/ 5 levels \"a\",\"b\",\"c\",\"d\",..: 2 2 1 4 5 1 4 3 1 5 ...\n\nhead(exam)\n\n\n  x1        x2         x3 x4 x5 x6 x7\n1  1  2.554297  0.1939687  a  c  c  b\n2  2  4.554297  2.1939687  b  h  d  b\n3  3  9.554297  4.1939687  c  f  f  a\n4  4 12.554297  6.1939687  d  e  g  d\n5  5 10.554297  3.1939687  e  g  f  e\n6  6  6.554297 10.1939687  f  l  h  a\n\n\n# default case\nexam_01 <- treatment_corr(exam)\nhead(exam_01)\n\n\n         x2         x3 x6 x7\n1  2.554297  0.1939687  c  b\n2  4.554297  2.1939687  d  b\n3  9.554297  4.1939687  f  a\n4 12.554297  6.1939687  g  d\n5 10.554297  3.1939687  f  e\n6  6.554297 10.1939687  h  a\n\n\n# not removing variables\ntreatment_corr(exam, treat = FALSE)\n\n# Set a threshold to detecting variables when correlation greater then 0.9\ntreatment_corr(exam, corr_thres = 0.9, treat = FALSE)\n\n# not verbose mode\nexam_02 <- treatment_corr(exam, verbose = FALSE)\nhead(exam_02)\n\n\n         x2         x3 x6 x7\n1  2.554297  0.1939687  c  b\n2  4.554297  2.1939687  d  b\n3  9.554297  4.1939687  f  a\n4 12.554297  6.1939687  g  d\n5 10.554297  3.1939687  f  e\n6  6.554297 10.1939687  h  a\n\nremove variables whose strong correlation : x1, x4, x5 are removed.\n\n\n\n",
      "last_modified": "2021-11-30T08:24:14+09:00"
    },
    {
      "path": "index.html",
      "title": "Introduce alookr",
      "description": "Introduce alookr package for data cleansing, spliting and modeling\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-30",
      "contents": "\n\nContents\nOverview\nInstall alookr\nUsage\nCleansing the dataset\nData: create example dataset\nClean dataset\nDiagnosis and removal of highly correlated variables\n\nSplit the data into a train set and a test set\nData: Credit Card Default Data\nSplit dataset\nCompare dataset\nExtract train/test dataset\n\nModeling and Evaluate, Predict\nData: Wisconsin Breast Cancer Data\nPreperation the data\nSplit data set\nHandling the imbalanced classes data with sampling_target()\nCleansing the dataset for classification modeling with cleanse()\nExtract test set for evaluation of the model with extract_set()\nBinary classification modeling with run_models()\nEvaluate the model\nPredict\n\n\nOverview\nBinary classification modeling with alookr.\nFeatures:\nClean and split data sets to train and test.\nCreate several representative models.\nEvaluate the performance of the model to select the best model.\nSupport the entire process of developing a binary classification model.\nThe name alookr comes from looking at the analytics process in the data analysis process.\nInstall alookr\nThe released version is available on CRAN. but not yet.\n\n\ninstall.packages(\"alookr\")\n\n\n\nOr you can get the development version without vignettes from GitHub:\n\n\ndevtools::install_github(\"choonghyunryu/alookr\")\n\n\n\nOr you can get the development version with vignettes from GitHub:\n\n\ninstall.packages(c(\"ISLR\", \"spelling\", \"mlbench\"))\ndevtools::install_github(\"choonghyunryu/alookr\", build_vignettes = TRUE)\n\n\n\nUsage\nalookr includes several vignette files, which we use throughout the documentation.\nProvided vignettes is as follows.\nCleansing the dataset\nSplit the data into a train set and a test set\nModeling and Evaluate, Predict\n\n\nbrowseVignettes(package = \"alookr\")\n\n\n\nCleansing the dataset\nData: create example dataset\nTo illustrate basic use of the alookr package, create the data_exam with sample function. The data_exam dataset include 5 variables.\nvariables are as follows.:\nid : character\nyear: character\ncount: numeric\nalpha : character\nflag : character\n\n\n# create sample dataset\nset.seed(123L)\nid <- sapply(1:1000, function(x)\n  paste(c(sample(letters, 5), x), collapse = \"\"))\n\nyear <- \"2018\"\n\nset.seed(123L)\ncount <- sample(1:10, size = 1000, replace = TRUE)\n\nset.seed(123L)\nalpha <- sample(letters, size = 1000, replace = TRUE)\n\nset.seed(123L)\nflag <- sample(c(\"Y\", \"N\"), size = 1000, prob = c(0.1, 0.9), replace = TRUE)\n\ndata_exam <- data.frame(id, year, count, alpha, flag, stringsAsFactors = FALSE)\n\n# structure of dataset\nstr(data_exam)\n\n\n'data.frame':   1000 obs. of  5 variables:\n $ id   : chr  \"osncj1\" \"rvket2\" \"nvesi3\" \"chgji4\" ...\n $ year : chr  \"2018\" \"2018\" \"2018\" \"2018\" ...\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: chr  \"o\" \"s\" \"n\" \"c\" ...\n $ flag : chr  \"N\" \"N\" \"N\" \"N\" ...\n\n# summary of dataset\nsummary(data_exam)\n\n\n      id                year               count       \n Length:1000        Length:1000        Min.   : 1.000  \n Class :character   Class :character   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median : 6.000  \n                                       Mean   : 5.698  \n                                       3rd Qu.: 8.000  \n                                       Max.   :10.000  \n    alpha               flag          \n Length:1000        Length:1000       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\nClean dataset\ncleanse() cleans up the dataset before fitting the classification model.\nThe function of cleanse() is as follows.:\nremove variables whose unique value is one\nremove variables with high unique rate\nconverts character variables to factor\nremove variables with missing value\nCleanse dataset with cleanse()\nFor example, we can cleanse all variables in data_exam:\n\n\nlibrary(alookr)\n\n# cleansing dataset\nnewDat <- cleanse(data_exam)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n• flag\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nremove variables whose unique value is one : The year variable has only one value, “2018”. Not needed when fitting the model. So it was removed.\nremove variables with high unique rate : If the number of levels of categorical data is very large, it is not suitable for classification model. In this case, it is highly likely to be an identifier of the data. So, remove the categorical (or character) variable with a high value of the unique rate defined as “number of levels / number of observations”.\nThe unique rate of the id variable with the number of levels of 1000 is 1. This variable is the object of the removal by identifier.\nThe unique rate of the alpha variable is 0.026 and this variable is also removed.\n\nconverts character variables to factor : The character type flag variable is converted to a factor type.\nFor example, we can not remove the categorical data that is removed by changing the threshold of the unique rate:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam, uniq_thres = 0.03)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n• flag\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nThe alpha variable was not removed.\nIf you do not want to apply a unique rate, you can set the value of the uniq argument to FALSE.:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam, uniq = FALSE)\n\n\n── Checking character variables ─────────────────────── categorical data ──\n• id\n• year\n• alpha\n• flag\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  5 variables:\n $ id   : Factor w/ 1000 levels \"ablnc282\",\"abqym54\",..: 594 715 558 94 727 270 499 882 930 515 ...\n $ year : Factor w/ 1 level \"2018\": 1 1 1 1 1 1 1 1 1 1 ...\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nIf you do not want to force type conversion of a character variable to factor, you can set the value of the char argument to FALSE.:\n\n\n# cleansing dataset\nnewDat <- cleanse(data_exam, char = FALSE)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: chr  \"o\" \"s\" \"n\" \"c\" ...\n $ flag : chr  \"N\" \"N\" \"N\" \"N\" ...\n\nIf you want to remove a variable that contains missing values, specify the value of the missing argument as TRUE. The following example removes the flag variable that contains the missing value.\n\n\ndata_exam$flag[1] <- NA \n\n# cleansing dataset\nnewDat <- cleanse(data_exam, missing = TRUE)\n\n\n── Checking missing value ────────────────────────────────── included NA ──\n• flag\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n\n# structure of cleansing dataset\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  2 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n\nDiagnosis and removal of highly correlated variables\nIn the linear model, there is a multicollinearity if there is a strong correlation between independent variables. So it is better to remove one variable from a pair of variables where the correlation exists.\nEven if it is not a linear model, removing one variable from a strongly correlated pair of variables can also reduce the overhead of the operation. It is also easy to interpret the model.\nCleanse dataset with treatment_corr()\ntreatment_corr() diagnose pairs of highly correlated variables or remove on of them.\ntreatment_corr() calculates correlation coefficient of pearson for numerical variable, and correlation coefficient of spearman for categorical variable.\nFor example, we can diagnosis and removal of highly correlated variables:\n\n\n# numerical variable\nx1 <- 1:100\nset.seed(12L)\nx2 <- sample(1:3, size = 100, replace = TRUE) * x1 + rnorm(1)\nset.seed(1234L)\nx3 <- sample(1:2, size = 100, replace = TRUE) * x1 + rnorm(1)\n\n# categorical variable\nx4 <- factor(rep(letters[1:20], time = 5))\nset.seed(100L)\nx5 <- factor(rep(letters[1:20 + sample(1:6, size = 20, replace = TRUE)], time = 5))\nset.seed(200L)\nx6 <- factor(rep(letters[1:20 + sample(1:3, size = 20, replace = TRUE)], time = 5))\nset.seed(300L)\nx7 <- factor(sample(letters[1:5], size = 100, replace = TRUE))\n\nexam <- data.frame(x1, x2, x3, x4, x5, x6, x7)\nstr(exam)\n\n\n'data.frame':   100 obs. of  7 variables:\n $ x1: int  1 2 3 4 5 6 7 8 9 10 ...\n $ x2: num  2.55 4.55 9.55 12.55 10.55 ...\n $ x3: num  0.194 2.194 4.194 6.194 3.194 ...\n $ x4: Factor w/ 20 levels \"a\",\"b\",\"c\",\"d\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ x5: Factor w/ 13 levels \"c\",\"e\",\"f\",\"g\",..: 1 5 3 2 4 7 6 8 9 8 ...\n $ x6: Factor w/ 15 levels \"c\",\"d\",\"f\",\"g\",..: 1 2 3 4 3 5 6 7 8 9 ...\n $ x7: Factor w/ 5 levels \"a\",\"b\",\"c\",\"d\",..: 2 2 1 4 5 1 4 3 1 5 ...\n\nhead(exam)\n\n\n  x1        x2         x3 x4 x5 x6 x7\n1  1  2.554297  0.1939687  a  c  c  b\n2  2  4.554297  2.1939687  b  h  d  b\n3  3  9.554297  4.1939687  c  f  f  a\n4  4 12.554297  6.1939687  d  e  g  d\n5  5 10.554297  3.1939687  e  g  f  e\n6  6  6.554297 10.1939687  f  l  h  a\n\n# default case\nexam_01 <- treatment_corr(exam)\nhead(exam_01)\n\n\n         x2         x3 x6 x7\n1  2.554297  0.1939687  c  b\n2  4.554297  2.1939687  d  b\n3  9.554297  4.1939687  f  a\n4 12.554297  6.1939687  g  d\n5 10.554297  3.1939687  f  e\n6  6.554297 10.1939687  h  a\n\n# not removing variables\ntreatment_corr(exam, treat = FALSE)\n\n# Set a threshold to detecting variables when correlation greater then 0.9\ntreatment_corr(exam, corr_thres = 0.9, treat = FALSE)\n\n# not verbose mode\nexam_02 <- treatment_corr(exam, verbose = FALSE)\nhead(exam_02)\n\n\n         x2         x3 x6 x7\n1  2.554297  0.1939687  c  b\n2  4.554297  2.1939687  d  b\n3  9.554297  4.1939687  f  a\n4 12.554297  6.1939687  g  d\n5 10.554297  3.1939687  f  e\n6  6.554297 10.1939687  h  a\n\nremove variables whose strong correlation : x1, x4, x5 are removed.\nSplit the data into a train set and a test set\nData: Credit Card Default Data\nDefault of ISLR package is a simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\nA data frame with 10000 observations on the following 4 variables.:\ndefault : factor. A factor with levels No and Yes indicating whether the customer defaulted on their debt\nstudent: factor. A factor with levels No and Yes indicating whether the customer is a student\nbalance: numeric. The average balance that the customer has remaining on their credit card after making their monthly payment\nincome : numeric. Income of customer\n\n\n# Credit Card Default Data\nhead(ISLR::Default)\n\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n# structure of dataset\nstr(ISLR::Default)\n\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\n# summary of dataset\nsummary(ISLR::Default)\n\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nSplit dataset\nsplit_by() splits the data.frame or tbl_df into a training set and a test set.\nSplit dataset with split_by()\nThe split_df class is created, which contains the split information and criteria to separate the training and the test set.\n\n\nlibrary(alookr)\nlibrary(dplyr)\n\n# Generate data for the example\nsb <- ISLR::Default %>%\n  split_by(default, seed = 6534)\n\nsb\n\n\n# A tibble: 10,000 x 5\n# Groups:   split_flag [2]\n  default student balance income split_flag\n  <fct>   <fct>     <dbl>  <dbl> <chr>     \n1 No      No         730. 44362. train     \n2 No      Yes        817. 12106. train     \n3 No      No        1074. 31767. train     \n4 No      No         529. 35704. train     \n# … with 9,996 more rows\n\nThe attributes of the split_df class are as follows.:\nsplit_seed : integer. random seed used for splitting\ntarget : character. the name of the target variable\nbinary : logical. whether the target variable is binary class\nminority : character. the name of the minority class\nmajority : character. the name of the majority class\nminority_rate : numeric. the rate of the minority class\nmajority_rate : numeric. the rate of the majority class\n\n\nattr_names <- names(attributes(sb))\nattr_names\n\n\n [1] \"names\"         \"row.names\"     \"groups\"        \"class\"        \n [5] \"split_seed\"    \"target\"        \"binary\"        \"minority\"     \n [9] \"majority\"      \"minority_rate\" \"majority_rate\"\n\nsb_attr <- attributes(sb)\n\n# The third property, row.names, is excluded from the output because its length is very long.\nsb_attr[!attr_names %in% \"row.names\"]\n\n\n$names\n[1] \"default\"    \"student\"    \"balance\"    \"income\"     \"split_flag\"\n\n$groups\n# A tibble: 2 x 2\n  split_flag       .rows\n  <chr>      <list<int>>\n1 test           [3,000]\n2 train          [7,000]\n\n$class\n[1] \"split_df\"   \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$split_seed\n[1] 6534\n\n$target\n  default \n\"default\" \n\n$binary\n[1] TRUE\n\n$minority\n[1] \"Yes\"\n\n$majority\n[1] \"No\"\n\n$minority_rate\n   Yes \n0.0333 \n\n$majority_rate\n    No \n0.9667 \n\nsummary() summarizes the information of two datasets splitted by split_by().\n\n\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  6534 \n + split data            \n    - train set count :  7000 \n    - test set count  :  3000 \n + target variable    :  default \n    - minority class  :  Yes (0.033300)\n    - majority class  :  No (0.966700)\n\nCompare dataset\nTrain data and test data should be similar. If the two datasets are not similar, the performance of the predictive model may be reduced.\nalookr provides a function to compare the similarity between train dataset and test dataset.\nIf the two data sets are not similar, the train dataset and test dataset should be splitted again from the original data.\nComparison of categorical variables with compare_target_category()\nCompare the statistics of the categorical variables of the train set and test set included in the “split_df” class.\n\n\nsb %>%\n  compare_target_category()\n\n\n# A tibble: 4 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 default  No    96.7  96.7   0.00476\n2 default  Yes    3.33  3.33  0.00476\n3 student  No    70.0  71.8   1.77   \n4 student  Yes   30.0  28.2   1.77   \n\n# compare variables that are character data types.\nsb %>%\n  compare_target_category(add_character = TRUE)\n\n\n# A tibble: 4 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 default  No    96.7  96.7   0.00476\n2 default  Yes    3.33  3.33  0.00476\n3 student  No    70.0  71.8   1.77   \n4 student  Yes   30.0  28.2   1.77   \n\n# display marginal\nsb %>%\n  compare_target_category(margin = TRUE)\n\n\n# A tibble: 6 x 5\n  variable level    train   test abs_diff\n  <chr>    <fct>    <dbl>  <dbl>    <dbl>\n1 default  No       96.7   96.7   0.00476\n2 default  Yes       3.33   3.33  0.00476\n3 default  <Total> 100    100     0.00952\n4 student  No       70.0   71.8   1.77   \n# … with 2 more rows\n\n# student variable only\nsb %>%\n  compare_target_category(student)\n\n\n# A tibble: 2 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 student  No     70.0  71.8     1.77\n2 student  Yes    30.0  28.2     1.77\n\nsb %>%\n  compare_target_category(student, margin = TRUE)\n\n\n# A tibble: 3 x 5\n  variable level   train  test abs_diff\n  <chr>    <fct>   <dbl> <dbl>    <dbl>\n1 student  No       70.0  71.8     1.77\n2 student  Yes      30.0  28.2     1.77\n3 student  <Total> 100   100       3.54\n\ncompare_target_category() returns tbl_df, where the variables have the following.:\nvariable : character. categorical variable name\nlevel : factor. level of categorical variables\ntrain : numeric. the relative frequency of the level in the train set\ntest : numeric. the relative frequency of the level in the test set\nabs_diff : numeric. the absolute value of the difference between two relative frequencies\nComparison of numeric variables with compare_target_numeric()\nCompare the statistics of the numerical variables of the train set and test set included in the “split_df” class.\n\n\nsb %>%\n  compare_target_numeric()\n\n\n# A tibble: 2 x 7\n  variable train_mean test_mean train_sd test_sd train_z test_z\n  <chr>         <dbl>     <dbl>    <dbl>   <dbl>   <dbl>  <dbl>\n1 balance        836.      834.     487.    477.    1.72   1.75\n2 income       33446.    33684.   13437.  13101.    2.49   2.57\n\n# balance variable only\nsb %>%\n  compare_target_numeric(balance)\n\n\n# A tibble: 1 x 7\n  variable train_mean test_mean train_sd test_sd train_z test_z\n  <chr>         <dbl>     <dbl>    <dbl>   <dbl>   <dbl>  <dbl>\n1 balance        836.      834.     487.    477.    1.72   1.75\n\ncompare_target_numeric() returns tbl_df, where the variables have the following.:\nvariable : character. numeric variable name\ntrain_mean : numeric. arithmetic mean of train set\ntest_mean : numeric. arithmetic mean of test set\ntrain_sd : numeric. standard deviation of train set\ntest_sd : numeric. standard deviation of test set\ntrain_z : numeric. the arithmetic mean of the train set divided by the standard deviation\ntest_z : numeric. the arithmetic mean of the test set divided by the standard deviation\nComparison plot with compare_plot()\nPlot compare information of the train set and test set included in the “split_df” class.\n\n\n# income variable only\nsb %>%\n  compare_plot(\"income\")\n\n\n\n# all varibales\nsb %>%\n  compare_plot()\n\n\n\n\nDiagnosis of train set and test set with compare_diag()\nDiagnosis of similarity between datasets splitted by train set and set included in the “split_df” class.\n\n\ndefaults <- ISLR::Default\ndefaults$id <- seq(NROW(defaults))\n\nset.seed(1)\ndefaults[sample(seq(NROW(defaults)), 3), \"student\"] <- NA\nset.seed(2)\ndefaults[sample(seq(NROW(defaults)), 10), \"balance\"] <- NA\n\nsb_2 <- defaults %>%\n  split_by(default)\n\nsb_2 %>%\n  compare_diag()\n\n\n$missing_value\n# A tibble: 3 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429       NA     \n2 balance                 8         0.114        NA     \n3 balance                 2        NA             0.0667\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 0 x 5\n# … with 5 variables: variables <chr>, train_uniqcount <int>,\n#   train_uniqrate <dbl>, test_uniqcount <int>, test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\nsb_2 %>%\n  compare_diag(add_character = TRUE)\n\n\n$missing_value\n# A tibble: 3 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429       NA     \n2 balance                 8         0.114        NA     \n3 balance                 2        NA             0.0667\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 0 x 5\n# … with 5 variables: variables <chr>, train_uniqcount <int>,\n#   train_uniqrate <dbl>, test_uniqcount <int>, test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\nsb_2 %>%\n  compare_diag(uniq_thres = 0.0005)\n\n\n$missing_value\n# A tibble: 3 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429       NA     \n2 balance                 8         0.114        NA     \n3 balance                 2        NA             0.0667\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 2 x 5\n  variables train_uniqcount train_uniqrate test_uniqcount\n  <chr>               <int>          <dbl>          <int>\n1 default                NA             NA              2\n2 student                NA             NA              2\n# … with 1 more variable: test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\nExtract train/test dataset\nIf you compare the train set with the test set and find that the two datasets are similar, extract the data from the split_df object.\nExtract train set or test set with extract_set()\nExtract train set or test set from split_df class object.\n\n\ntrain <- sb %>%\n  extract_set(set = \"train\")\n\ntest <- sb %>%\n  extract_set(set = \"test\")\n\ndim(train)\n\n\n[1] 7000    4\n\ndim(test)\n\n\n[1] 3000    4\n\nExtract the data to fit the model with sampling_target()\nIn a target class, the ratio of the majority class to the minority class is not similar and the ratio of the minority class is very small, which is called the imbalanced class.\nIf target variable is an imbalanced class, the characteristics of the majority class are actively reflected in the model. This model implies an error in predicting the minority class as the majority class. So we have to make the train dataset a balanced class.\nsampling_target() performs sampling on the train set of split_df to resolve the imbalanced class.\n\n\n# under-sampling with random seed\nunder <- sb %>%\n  sampling_target(seed = 1234L)\n\nunder %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        233\n2 Yes       233\n\n# under-sampling with random seed, and minority class frequency is 40%\nunder40 <- sb %>%\n  sampling_target(seed = 1234L, perc = 40)\n\nunder40 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        349\n2 Yes       233\n\n# over-sampling with random seed\nover <- sb %>%\n  sampling_target(method = \"ubOver\", seed = 1234L)\n\nover %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       6767\n2 Yes      6767\n\n# over-sampling with random seed, and k = 10\nover10 <- sb %>%\n  sampling_target(method = \"ubOver\", seed = 1234L, k = 10)\n\nover10 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       6767\n2 Yes      2330\n\n# SMOTE with random seed\nsmote <- sb %>%\n  sampling_target(method = \"ubSMOTE\", seed = 1234L)\n\nsmote %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        932\n2 Yes       699\n\n# SMOTE with random seed, and perc.under = 250\nsmote250 <- sb %>%\n  sampling_target(method = \"ubSMOTE\", seed = 1234L, perc.under = 250)\n\nsmote250 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       1165\n2 Yes       699\n\nThe argument that specifies the sampling method in sampling_target () is method. “ubUnder” is under-sampling, and “ubOver” is over-sampling, “ubSMOTE” is SMOTE(Synthetic Minority Over-sampling TEchnique).\nModeling and Evaluate, Predict\nData: Wisconsin Breast Cancer Data\nBreastCancer of mlbench package is a breast cancer data. The objective is to identify each of a number of benign or malignant classes.\nA data frame with 699 observations on 11 variables, one being a character variable, 9 being ordered or nominal, and 1 target class.:\nId : character. Sample code number\nCl.thickness : ordered factor. Clump Thickness\nCell.size : ordered factor. Uniformity of Cell Size\nCell.shape : ordered factor. Uniformity of Cell Shape\nMarg.adhesion : ordered factor. Marginal Adhesion\nEpith.c.size : ordered factor. Single Epithelial Cell Size\nBare.nuclei : factor. Bare Nuclei\nBl.cromatin : factor. Bland Chromatin\nNormal.nucleoli : factor. Normal Nucleoli\nMitoses : factor. Mitoses\nClass : factor. Class. level is benign and malignant.\n\n\nlibrary(mlbench)\ndata(BreastCancer)\n\n# class of each variables\nsapply(BreastCancer, function(x) class(x)[1])\n\n\n             Id    Cl.thickness       Cell.size      Cell.shape \n    \"character\"       \"ordered\"       \"ordered\"       \"ordered\" \n  Marg.adhesion    Epith.c.size     Bare.nuclei     Bl.cromatin \n      \"ordered\"       \"ordered\"        \"factor\"        \"factor\" \nNormal.nucleoli         Mitoses           Class \n       \"factor\"        \"factor\"        \"factor\" \n\nPreperation the data\nPerform data preprocessing as follows.:\nFind and imputate variables that contain missing values.\nSplit the data into a train set and a test set.\nTo solve the imbalanced class, perform sampling in the train set of raw data.\nCleansing the dataset for classification modeling.\nFix the missing value with dlookr::imputate_na()\nfind the variables that include missing value. and imputate the missing value using imputate_na() in dlookr package.\n\n\nlibrary(dlookr)\nlibrary(dplyr)\n\n# variable that have a missing value\ndiagnose(BreastCancer) %>%\n  filter(missing_count > 0)\n\n\n# A tibble: 1 x 6\n  variables   types  missing_count missing_percent unique_count\n  <chr>       <chr>          <int>           <dbl>        <int>\n1 Bare.nuclei factor            16            2.29           11\n# … with 1 more variable: unique_rate <dbl>\n\n# imputation of missing value\nbreastCancer <- BreastCancer %>%\n  mutate(Bare.nuclei = imputate_na(BreastCancer, Bare.nuclei, Class,\n                         method = \"mice\", no_attrs = TRUE, print_flag = FALSE))\n\n\n\nSplit data set\nSplits the dataset into a train set and a test set with split_by()\nsplit_by() in the alookr package splits the dataset into a train set and a test set.\nThe ratio argument of the split_by() function specifies the ratio of the train set.\nsplit_by() creates a class object named split_df.\n\n\nlibrary(alookr)\n\n# split the data into a train set and a test set by default arguments\nsb <- breastCancer %>%\n  split_by(target = Class)\n\n# show the class name\nclass(sb)\n\n\n[1] \"split_df\"   \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# split the data into a train set and a test set by ratio = 0.6\ntmp <- breastCancer %>%\n  split_by(Class, ratio = 0.6)\n\n\n\nThe summary() function displays the following useful information about the split_df object:\nrandom seed : The random seed is the random seed used internally to separate the data\nsplit data : Information of splited data\ntrain set count : number of train set\ntest set count : number of test set\n\ntarget variable : Target variable name\nminority class : name and ratio(In parentheses) of minority class\nmajority class : name and ratio(In parentheses) of majority class\n\n\n\n# summary() display the some information\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  14818 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\n# summary() display the some information\nsummary(tmp)\n\n\n** Split train/test set information **\n + random seed        :  44115 \n + split data            \n    - train set count :  419 \n    - test set count  :  280 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\nCheck missing levels in the train set\nIn the case of categorical variables, when a train set and a test set are separated, a specific level may be missing from the train set.\nIn this case, there is no problem when fitting the model, but an error occurs when predicting with the model you created. Therefore, preprocessing is performed to avoid missing data preprocessing.\nIn the following example, fortunately, there is no categorical variable that contains the missing levels in the train set.\n\n\n# list of categorical variables in the train set that contain missing levels\nnolevel_in_train <- sb %>%\n  compare_target_category() %>% \n  filter(is.na(train)) %>% \n  select(variable) %>% \n  unique() %>% \n  pull\n\nnolevel_in_train\n\n\ncharacter(0)\n\n# if any of the categorical variables in the train set contain a missing level, \n# split them again.\nwhile (length(nolevel_in_train) > 0) {\n  sb <- breastCancer %>%\n    split_by(Class)\n\n  nolevel_in_train <- sb %>%\n    compare_target_category() %>% \n    filter(is.na(train)) %>% \n    select(variable) %>% \n    unique() %>% \n    pull\n}\n\n\n\nHandling the imbalanced classes data with sampling_target()\nIssue of imbalanced classes data\nImbalanced classes(levels) data means that the number of one level of the frequency of the target variable is relatively small. In general, the proportion of positive classes is relatively small. For example, in the model of predicting spam, the class of interest spam is less than non-spam.\nImbalanced classes data is a common problem in machine learning classification.\ntable() and prop.table() are traditionally useful functions for diagnosing imbalanced classes data. However, alookr’s summary() is simpler and provides more information.\n\n\n# train set frequency table - imbalanced classes data\ntable(sb$Class)\n\n\n\n   benign malignant \n      458       241 \n\n# train set relative frequency table - imbalanced classes data\nprop.table(table(sb$Class))\n\n\n\n   benign malignant \n0.6552217 0.3447783 \n\n# using summary function - imbalanced classes data\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  14818 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\nHandling the imbalanced classes data\nMost machine learning algorithms work best when the number of samples in each class are about equal. And most algorithms are designed to maximize accuracy and reduce error. So, we requre handling an imbalanced class problem.\nsampling_target() performs sampling to solve an imbalanced classes data problem.\nResampling - oversample minority class\nOversampling can be defined as adding more copies of the minority class.\nOversampling is performed by specifying “ubOver” in the method argument of the sampling_target() function.\n\n\n# to balanced by over sampling\ntrain_over <- sb %>%\n  sampling_target(method = \"ubOver\")\n\n# frequency table \ntable(train_over$Class)\n\n\n\n   benign malignant \n      319       319 \n\nResampling - undersample majority class\nUndersampling can be defined as removing some observations of the majority class.\nUndersampling is performed by specifying “ubUnder” in the method argument of the sampling_target() function.\n\n\n# to balanced by under sampling\ntrain_under <- sb %>%\n  sampling_target(method = \"ubUnder\")\n\n# frequency table \ntable(train_under$Class)\n\n\n\n   benign malignant \n      170       170 \n\nGenerate synthetic samples - SMOTE\nSMOTE(Synthetic Minority Oversampling Technique) uses a nearest neighbors algorithm to generate new and synthetic data.\nSMOTE is performed by specifying “ubSMOTE” in the method argument of the sampling_target() function.\n\n\n# to balanced by SMOTE\ntrain_smote <- sb %>%\n  sampling_target(seed = 1234L, method = \"ubSMOTE\")\n\n# frequency table \ntable(train_smote$Class)\n\n\n\n   benign malignant \n      680       510 \n\nCleansing the dataset for classification modeling with cleanse()\nThe cleanse() cleanse the dataset for classification modeling.\nThis function is useful when fit the classification model. This function does the following.:\nRemove the variable with only one value.\nAnd remove variables that have a unique number of values relative to the number of observations for a character or categorical variable.\nIn this case, it is a variable that corresponds to an identifier or an identifier.\n\nAnd converts the character to factor.\nIn this example, The cleanse() function removed a variable ID with a high unique rate.\n\n\n# clean the training set\ntrain <- train_smote %>%\n  cleanse\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 437(0.367226890756303)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\nExtract test set for evaluation of the model with extract_set()\n\n\n# extract test set\ntest <- sb %>%\n  extract_set(set = \"test\")\n\n\n\nBinary classification modeling with run_models()\nrun_models() performs some representative binary classification modeling using split_df object created by split_by().\nrun_models() executes the process in parallel when fitting the model. However, it is not supported in MS-Windows operating system and RStudio environment.\nCurrently supported algorithms are as follows.:\nlogistic : logistic regression using stats package\nrpart : Recursive Partitioning Trees using rpart package\nctree : Conditional Inference Trees using party package\nrandomForest :Classification with Random Forest using randomForest package\nranger : A Fast Implementation of Random Forests using ranger package\nrun_models() returns a model_df class object.\nThe model_df class object contains the following variables.:\nstep : character. The current stage in the classification modeling process.\nFor objects created with run_models(), the value of the variable is “1.Fitted”.\n\nmodel_id : model identifiers\ntarget : name of target variable\npositive : positive class in target variable\nfitted_model : list. Fitted model object by model_id’s algorithms\n\n\nresult <- train %>% \n  run_models(target = \"Class\", positive = \"malignant\")\nresult\n\n\n# A tibble: 7 x 7\n  step     model_id    target is_factor positive negative fitted_model\n  <chr>    <chr>       <chr>  <lgl>     <chr>    <chr>    <list>      \n1 1.Fitted logistic    Class  TRUE      maligna… benign   <glm>       \n2 1.Fitted rpart       Class  TRUE      maligna… benign   <rpart>     \n3 1.Fitted ctree       Class  TRUE      maligna… benign   <BinaryTr>  \n4 1.Fitted randomFore… Class  TRUE      maligna… benign   <rndmFrs.>  \n# … with 3 more rows\n\nEvaluate the model\nEvaluate the predictive performance of fitted models.\nPredict test set using fitted model with run_predict()\nrun_predict() predict the test set using model_df class fitted by run_models().\nrun_predict () is executed in parallel when predicting by model. However, it is not supported in MS-Windows operating system and RStudio environment.\nThe model_df class object contains the following variables.:\nstep : character. The current stage in the classification modeling process.\nFor objects created with run_predict(), the value of the variable is “2.Predicted”.\n\nmodel_id : character. Type of fit model.\ntarget : character. Name of target variable.\npositive : character. Level of positive class of binary classification.\nfitted_model : list. Fitted model object by model_id’s algorithms.\npredicted : result of predcit by each models\n\n\npred <- result %>%\n  run_predict(test)\npred\n\n\n# A tibble: 7 x 8\n  step      model_id   target is_factor positive negative fitted_model\n  <chr>     <chr>      <chr>  <lgl>     <chr>    <chr>    <list>      \n1 2.Predic… logistic   Class  TRUE      maligna… benign   <glm>       \n2 2.Predic… rpart      Class  TRUE      maligna… benign   <rpart>     \n3 2.Predic… ctree      Class  TRUE      maligna… benign   <BinaryTr>  \n4 2.Predic… randomFor… Class  TRUE      maligna… benign   <rndmFrs.>  \n# … with 3 more rows, and 1 more variable: predicted <list>\n\nCalculate the performance metric with run_performance()\nrun_performance() calculate the performance metric of model_df class predicted by run_predict().\nrun_performance () is performed in parallel when calculating the performance evaluation index. However, it is not supported in MS-Windows operating system and RStudio environment.\nThe model_df class object contains the following variables.:\nstep : character. The current stage in the classification modeling process.\nFor objects created with run_performance(), the value of the variable is “3.Performanced”.\n\nmodel_id : character. Type of fit model.\ntarget : character. Name of target variable.\npositive : character. Level of positive class of binary classification.\nfitted_model : list. Fitted model object by model_id’s algorithms\npredicted : list. Predicted value by individual model. Each value has a predict_class class object.\nperformance : list. Calculate metrics by individual model. Each value has a numeric vector.\n\n\n# Calculate performace metrics.\nperf <- run_performance(pred)\nperf\n\n\n# A tibble: 7 x 7\n  step    model_id  target positive fitted_model predicted performance\n  <chr>   <chr>     <chr>  <chr>    <list>       <list>    <list>     \n1 3.Perf… logistic  Class  maligna… <glm>        <fct [21… <dbl [15]> \n2 3.Perf… rpart     Class  maligna… <rpart>      <fct [21… <dbl [15]> \n3 3.Perf… ctree     Class  maligna… <BinaryTr>   <fct [21… <dbl [15]> \n4 3.Perf… randomFo… Class  maligna… <rndmFrs.>   <fct [21… <dbl [15]> \n# … with 3 more rows\n\nThe performance variable contains a list object, which contains 15 performance metrics:\nZeroOneLoss : Normalized Zero-One Loss(Classification Error Loss).\nAccuracy : Accuracy.\nPrecision : Precision.\nRecall : Recall.\nSensitivity : Sensitivity.\nSpecificity : Specificity.\nF1_Score : F1 Score.\nFbeta_Score : F-Beta Score.\nLogLoss : Log loss / Cross-Entropy Loss.\nAUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC).\nGini : Gini Coefficient.\nPRAUC : Area Under the Precision-Recall Curve (PR AUC).\nLiftAUC : Area Under the Lift Chart.\nGainAUC : Area Under the Gain Chart.\nKS_Stat : Kolmogorov-Smirnov Statistic.\n\n\n# Performance by analytics models\nperformance <- perf$performance\nnames(performance) <- perf$model_id\nperformance\n\n\n$logistic\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.04285714  0.95714286  0.90789474  0.97183099  0.97183099 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.94964029  0.93877551  0.93877551  1.43320622  0.96402878 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.94426994  0.01722128  1.17472123  0.80714286 92.86655183 \n\n$rpart\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.05238095  0.94761905  0.90540541  0.94366197  0.94366197 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.94964029  0.92413793  0.92413793  0.17443373  0.96879116 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.93677171  0.21381856  1.37318550  0.81029510 90.67788023 \n\n$ctree\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.04761905  0.95238095  0.90666667  0.95774648  0.95774648 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.94964029  0.93150685  0.93150685  0.33106801  0.97446550 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.95602391  0.64924384  1.87066074  0.81405097 90.73867666 \n\n$randomForest\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.03333333  0.96666667  0.92105263  0.98591549  0.98591549 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.95683453  0.95238095  0.95238095  0.10045446  0.99234978 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.98459824  0.75554241  1.88370599  0.82588867 95.71385145 \n\n$ranger\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.03333333  0.96666667  0.93243243  0.97183099  0.97183099 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.96402878  0.95172414  0.95172414  0.09562285  0.99260310 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.98520620  0.91071380  2.01842792  0.82605634 95.02482521 \n\n$xgboost\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.02857143  0.97142857  0.92207792  1.00000000  1.00000000 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.95683453  0.95945946  0.95945946  0.10796171  0.98895531 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.97730266  0.93589644  2.07536078  0.82364185 95.68345324 \n\n$lasso\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.03333333  0.96666667  0.93243243  0.97183099  0.97183099 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.96402878  0.95172414  0.95172414  0.09325757  0.99392036 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.98784071  0.97152128  2.05577211  0.82692824 95.02482521 \n\nIf you change the list object to tidy format, you’ll see the following at a glance:\n\n\n# Convert to matrix for compare performace.\nsapply(performance, \"c\")\n\n\n               logistic       rpart       ctree randomForest\nZeroOneLoss  0.04285714  0.05238095  0.04761905   0.03333333\nAccuracy     0.95714286  0.94761905  0.95238095   0.96666667\nPrecision    0.90789474  0.90540541  0.90666667   0.92105263\nRecall       0.97183099  0.94366197  0.95774648   0.98591549\nSensitivity  0.97183099  0.94366197  0.95774648   0.98591549\nSpecificity  0.94964029  0.94964029  0.94964029   0.95683453\nF1_Score     0.93877551  0.92413793  0.93150685   0.95238095\nFbeta_Score  0.93877551  0.92413793  0.93150685   0.95238095\nLogLoss      1.43320622  0.17443373  0.33106801   0.10045446\nAUC          0.96402878  0.96879116  0.97446550   0.99234978\nGini         0.94426994  0.93677171  0.95602391   0.98459824\nPRAUC        0.01722128  0.21381856  0.64924384   0.75554241\nLiftAUC      1.17472123  1.37318550  1.87066074   1.88370599\nGainAUC      0.80714286  0.81029510  0.81405097   0.82588867\nKS_Stat     92.86655183 90.67788023 90.73867666  95.71385145\n                 ranger     xgboost       lasso\nZeroOneLoss  0.03333333  0.02857143  0.03333333\nAccuracy     0.96666667  0.97142857  0.96666667\nPrecision    0.93243243  0.92207792  0.93243243\nRecall       0.97183099  1.00000000  0.97183099\nSensitivity  0.97183099  1.00000000  0.97183099\nSpecificity  0.96402878  0.95683453  0.96402878\nF1_Score     0.95172414  0.95945946  0.95172414\nFbeta_Score  0.95172414  0.95945946  0.95172414\nLogLoss      0.09562285  0.10796171  0.09325757\nAUC          0.99260310  0.98895531  0.99392036\nGini         0.98520620  0.97730266  0.98784071\nPRAUC        0.91071380  0.93589644  0.97152128\nLiftAUC      2.01842792  2.07536078  2.05577211\nGainAUC      0.82605634  0.82364185  0.82692824\nKS_Stat     95.02482521 95.68345324 95.02482521\n\ncompare_performance() return a list object(results of compared model performance). and list has the following components:\nrecommend_model : character. The name of the model that is recommended as the best among the various models.\ntop_count : numeric. The number of best performing performance metrics by model.\nmean_rank : numeric. Average of ranking individual performance metrics by model.\ntop_metric : list. The name of the performance metric with the best performance on individual performance metrics by model.\nIn this example, compare_performance() recommend the “ranger” model.\n\n\n# Compaire the Performance metrics of each model\ncomp_perf <- compare_performance(pred)\ncomp_perf\n\n\n$recommend_model\n[1] \"lasso\"\n\n$top_metric_count\n    logistic        rpart        ctree randomForest       ranger \n           0            0            0            1            2 \n     xgboost        lasso \n           5            7 \n\n$mean_rank\n    logistic        rpart        ctree randomForest       ranger \n    5.846154     6.461538     5.615385     2.961538     2.615385 \n     xgboost        lasso \n    2.423077     2.076923 \n\n$top_metric\n$top_metric$logistic\nNULL\n\n$top_metric$rpart\nNULL\n\n$top_metric$ctree\nNULL\n\n$top_metric$randomForest\n[1] \"KS_Stat\"\n\n$top_metric$ranger\n[1] \"Precision\"   \"Specificity\"\n\n$top_metric$xgboost\n[1] \"ZeroOneLoss\" \"Accuracy\"    \"Recall\"      \"F1_Score\"   \n[5] \"LiftAUC\"    \n\n$top_metric$lasso\n[1] \"Precision\"   \"Specificity\" \"LogLoss\"     \"AUC\"        \n[5] \"Gini\"        \"PRAUC\"       \"GainAUC\"    \n\nPlot the ROC curve with plot_performance()\ncompare_performance() plot ROC curve.\n\n\n# Plot ROC curve\nplot_performance(pred)\n\n\n\n\nTunning the cut-off\nIn general, if the prediction probability is greater than 0.5 in the binary classification model, it is predicted as positive class. In other words, 0.5 is used for the cut-off value. This applies to most model algorithms. However, in some cases, the performance can be tuned by changing the cut-off value.\nplot_cutoff () visualizes a plot to select the cut-off value, and returns the cut-off value.\n\n\npred_best <- pred %>% \n  filter(model_id == comp_perf$recommend_model) %>% \n  select(predicted) %>% \n  pull %>% \n  .[[1]] %>% \n  attr(\"pred_prob\")\n\ncutoff <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"mcc\")\n\n\n\ncutoff\n\n\n[1] 0.67\n\ncutoff2 <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"density\")\n\n\n\ncutoff2\n\n\n[1] 0.6928\n\ncutoff3 <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"prob\")\n\n\n\ncutoff3\n\n\n[1] 0.67\n\nPerformance comparison between prediction and tuned cut-off with performance_metric()\nCompare the performance of the original prediction with that of the tuned cut-off. Compare the cut-off with the non-cut model for the model with the best performance comp_perf$recommend_model.\n\n\ncomp_perf$recommend_model\n\n\n[1] \"lasso\"\n\n# extract predicted probability\nidx <- which(pred$model_id == comp_perf$recommend_model)\npred_prob <- attr(pred$predicted[[idx]], \"pred_prob\")\n\n# or, extract predicted probability using dplyr\npred_prob <- pred %>% \n  filter(model_id == comp_perf$recommend_model) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  attr(\"pred_prob\")\n\n# predicted probability\npred_prob  \n\n\n  [1] 0.0177546700 0.0189495146 0.0073209366 0.9999504268 0.0039579715\n  [6] 0.9759609573 0.0037722474 0.0094413466 0.0013023718 0.9998300604\n [11] 0.6869421814 0.9998437927 0.0050499002 0.9853830303 0.9999959690\n [16] 0.9985322958 0.7361849175 0.0140288334 0.0682727187 0.0078550012\n [21] 0.1095384107 0.9999980415 0.0017374737 0.0755990146 0.9979659465\n [26] 0.9977689915 0.0024806916 0.1080620891 0.9997972486 0.9998449540\n [31] 0.0022109726 0.0021955342 0.9999393703 0.0054353242 0.0061165962\n [36] 0.0020909665 0.0026223736 0.4237392464 0.9999997369 0.5406384477\n [41] 0.0020909665 0.0031319356 0.9983299419 0.9992408490 0.9815634328\n [46] 0.0094413466 0.0074795138 0.9983517936 0.0039579715 0.0031319356\n [51] 0.0141696118 0.9999996480 0.0401139369 0.9998134264 0.9999917474\n [56] 0.9810653114 0.9997727754 0.9889991960 0.6705777033 0.9935469003\n [61] 0.8472658494 0.0074795138 0.9997773947 0.9987562540 0.0388247105\n [66] 0.0031319356 0.0062207276 0.9085271128 0.9939917427 0.7895163765\n [71] 0.9849519886 0.9196712296 0.0426028985 0.9998039298 0.0031319356\n [76] 0.0032162770 0.0008728069 0.0013156114 0.9996734281 0.0062098409\n [81] 0.9057298818 0.9263995279 0.8963795408 0.0016539309 0.9536035605\n [86] 0.9970293842 0.9160360574 0.0008728069 0.9465787759 0.0031319356\n [91] 0.0013511064 0.0008728069 0.9999824436 0.0010416530 0.9290444876\n [96] 0.5564421412 0.9998324636 0.0093933413 0.9915908881 0.9999999440\n[101] 0.9999828204 0.0013023718 0.0121131663 0.0013156114 0.0128678209\n[106] 0.0008728069 0.0013511064 0.0640210122 0.0181963668 0.0006941589\n[111] 0.0029548680 0.0058004220 0.0015602932 0.9999995036 0.0021181708\n[116] 0.0013156114 0.1737380284 0.0016539309 0.1388836367 0.0038492779\n[121] 0.9999988443 0.0029063415 0.9922099008 0.0032346583 0.0008728069\n[126] 0.0050007783 0.0080339252 0.0050007783 0.3913793670 0.0074468436\n[131] 0.0082491823 0.0052869014 0.9999791314 0.9986223232 0.0020200395\n[136] 0.0039492874 0.0032346583 0.0059867407 0.9979949108 0.0184287353\n[141] 0.9999999995 0.9999033596 0.0061032049 0.0008728069 0.0061165962\n[146] 0.0061165962 0.0008728069 0.0094413466 0.0031498364 0.9993123673\n[151] 0.0032826657 0.0010973809 0.9878714634 0.0032346583 0.0089302538\n[156] 0.0039579715 0.0059973997 0.0061165962 0.0145468547 0.0020909665\n[161] 0.0061165962 0.0094413466 0.0010746645 0.0016633982 0.1163940612\n[166] 0.0025591913 0.0031319356 0.0140979073 0.0016539309 0.9969724915\n[171] 0.0020909665 0.9999955230 0.0115430114 0.9883739882 0.9572209234\n[176] 0.0061032049 0.9687516430 0.0262744005 0.0086594371 0.0013156114\n[181] 0.0061165962 0.9955646856 0.9999412377 0.0008728069 0.0091034557\n[186] 0.0061165962 0.0094413466 0.0008728069 0.0034200790 0.0137333220\n[191] 0.0008728069 0.9999961243 0.0061165962 0.0074795138 0.0094413466\n[196] 0.0008728069 0.0016539309 0.0024669885 0.0065334373 0.0008728069\n[201] 0.0048422415 0.0058840711 0.0008728069 0.9999999972 0.6887804967\n[206] 0.0014436550 0.0020909665 0.0120277662 0.0033144614 0.9932804070\n\n# compaire Accuracy\nperformance_metric(pred_prob, test$Class, \"malignant\", \"Accuracy\")\n\n\n[1] 0.9666667\n\nperformance_metric(pred_prob, test$Class, \"malignant\", \"Accuracy\",\n                   cutoff = cutoff)\n\n\n[1] 0.9761905\n\n# compaire Confusion Matrix\nperformance_metric(pred_prob, test$Class, \"malignant\", \"ConfusionMatrix\")\n\n\n           actual\npredict     benign malignant\n  benign       134         2\n  malignant      5        69\n\nperformance_metric(pred_prob, test$Class, \"malignant\", \"ConfusionMatrix\", \n                   cutoff = cutoff)\n\n\n           actual\npredict     benign malignant\n  benign       136         2\n  malignant      3        69\n\n# compaire F1 Score\nperformance_metric(pred_prob, test$Class, \"malignant\", \"F1_Score\")\n\n\n[1] 0.9517241\n\nperformance_metric(pred_prob, test$Class,  \"malignant\", \"F1_Score\", \n                   cutoff = cutoff)\n\n\n[1] 0.965035\n\nperformance_metric(pred_prob, test$Class,  \"malignant\", \"F1_Score\", \n                   cutoff = cutoff2)\n\n\n[1] 0.9571429\n\nIf the performance of the tuned cut-off is good, use it as a cut-off to predict positives.\nPredict\nIf you have selected a good model from several models, then perform the prediction with that model.\nCreate data set for predict\nCreate sample data for predicting by extracting 100 samples from the data set used in the previous under sampling example.\n\n\ndata_pred <- train_under %>% \n  cleanse \n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 331(0.973529411764706)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\nset.seed(1234L)\ndata_pred <- data_pred %>% \n  nrow %>% \n  seq %>% \n  sample(size = 50) %>% \n  data_pred[., ]\n\n\n\nPredict with alookr and dplyr\nDo a predict using the dplyr package. The last factor() function eliminates unnecessary information.\n\n\npred_actual <- pred %>%\n  filter(model_id == comp_perf$recommend_model) %>% \n  run_predict(data_pred) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  factor()\n\npred_actual\n\n\n [1] benign    benign    benign    malignant malignant benign   \n [7] benign    benign    malignant malignant benign    malignant\n[13] benign    benign    malignant malignant benign    benign   \n[19] malignant malignant benign    malignant benign    benign   \n[25] malignant benign    benign    benign    benign    benign   \n[31] malignant benign    benign    malignant benign    benign   \n[37] malignant benign    malignant benign    malignant benign   \n[43] benign    benign    malignant malignant malignant benign   \n[49] benign    benign   \nLevels: benign malignant\n\nIf you want to predict by cut-off, specify the cutoff argument in the run_predict() function as follows.:\nIn the example, there is no difference between the results of using cut-off and not.\n\n\npred_actual2 <- pred %>%\n  filter(model_id == comp_perf$recommend_model) %>% \n  run_predict(data_pred, cutoff) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  factor()\n\npred_actual2\n\n\n [1] benign    benign    benign    malignant malignant benign   \n [7] benign    benign    malignant malignant benign    malignant\n[13] benign    benign    malignant malignant benign    benign   \n[19] malignant malignant benign    malignant benign    benign   \n[25] malignant benign    benign    benign    benign    benign   \n[31] malignant benign    benign    malignant benign    benign   \n[37] malignant benign    malignant benign    malignant benign   \n[43] benign    benign    malignant malignant malignant benign   \n[49] benign    benign   \nLevels: benign malignant\n\nsum(pred_actual != pred_actual2)\n\n\n[1] 0\n\n\n\n\n",
      "last_modified": "2021-11-30T08:26:15+09:00"
    },
    {
      "path": "modeling.html",
      "title": "Classification Modeling",
      "description": "Modeling and Evaluate, Predict for binary classification\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-30",
      "contents": "\n\nContents\nPreface\nData: Wisconsin Breast Cancer Data\nPreperation the data\nFix the missing value with dlookr::imputate_na()\n\nSplit data set\nSplits the dataset into a train set and a test set with split_by()\nCheck missing levels in the train set\n\nHandling the imbalanced classes data with sampling_target()\nIssue of imbalanced classes data\nHandling the imbalanced classes data\nResampling - oversample minority class\nResampling - undersample majority class\nGenerate synthetic samples - SMOTE\n\nCleansing the dataset for classification modeling with cleanse()\nExtract test set for evaluation of the model with extract_set()\nBinary classification modeling with run_models()\nEvaluate the model\nPredict test set using fitted model with run_predict()\nCalculate the performance metric with run_performance()\nPlot the ROC curve with plot_performance()\nTunning the cut-off\nPerformance comparison between prediction and tuned cut-off with performance_metric()\n\nPredict\nCreate data set for predict\nPredict with alookr and dplyr\n\n\n\n\n\nPreface\nOnce the data set is ready for model development, the model is fitted, predicted and evaluated in the following ways:\nCleansing the dataset\nSplit the data into a train set and a test set\nModeling and Evaluate, Predict\nModeling\nBinary classification modeling\n\nEvaluate the model\nPredict test set using fitted model\nCalculate the performance metric\nPlot the ROC curve\nTunning the cut-off\n\nPredict\nPredict\nPredict with cut-off\n\n\nThe alookr package makes these steps fast and easy:\nData: Wisconsin Breast Cancer Data\nBreastCancer of mlbench package is a breast cancer data. The objective is to identify each of a number of benign or malignant classes.\nA data frame with 699 observations on 11 variables, one being a character variable, 9 being ordered or nominal, and 1 target class.:\nId : character. Sample code number\nCl.thickness : ordered factor. Clump Thickness\nCell.size : ordered factor. Uniformity of Cell Size\nCell.shape : ordered factor. Uniformity of Cell Shape\nMarg.adhesion : ordered factor. Marginal Adhesion\nEpith.c.size : ordered factor. Single Epithelial Cell Size\nBare.nuclei : factor. Bare Nuclei\nBl.cromatin : factor. Bland Chromatin\nNormal.nucleoli : factor. Normal Nucleoli\nMitoses : factor. Mitoses\nClass : factor. Class. level is benign and malignant.\n\n\nlibrary(mlbench)\ndata(BreastCancer)\n\n# class of each variables\nsapply(BreastCancer, function(x) class(x)[1])\n\n\n             Id    Cl.thickness       Cell.size      Cell.shape \n    \"character\"       \"ordered\"       \"ordered\"       \"ordered\" \n  Marg.adhesion    Epith.c.size     Bare.nuclei     Bl.cromatin \n      \"ordered\"       \"ordered\"        \"factor\"        \"factor\" \nNormal.nucleoli         Mitoses           Class \n       \"factor\"        \"factor\"        \"factor\" \n\nPreperation the data\nPerform data preprocessing as follows.:\nFind and imputate variables that contain missing values.\nSplit the data into a train set and a test set.\nTo solve the imbalanced class, perform sampling in the train set of raw data.\nCleansing the dataset for classification modeling.\nFix the missing value with dlookr::imputate_na()\nfind the variables that include missing value. and imputate the missing value using imputate_na() in dlookr package.\n\n\nlibrary(dlookr)\nlibrary(dplyr)\n\n# variable that have a missing value\ndiagnose(BreastCancer) %>%\n  filter(missing_count > 0)\n\n\n# A tibble: 1 x 6\n  variables   types  missing_count missing_percent unique_count\n  <chr>       <chr>          <int>           <dbl>        <int>\n1 Bare.nuclei factor            16            2.29           11\n# … with 1 more variable: unique_rate <dbl>\n\n\n# imputation of missing value\nbreastCancer <- BreastCancer %>%\n  mutate(Bare.nuclei = imputate_na(BreastCancer, Bare.nuclei, Class,\n                         method = \"mice\", no_attrs = TRUE, print_flag = FALSE))\n\n\n\nSplit data set\nSplits the dataset into a train set and a test set with split_by()\nsplit_by() in the alookr package splits the dataset into a train set and a test set.\nThe ratio argument of the split_by() function specifies the ratio of the train set.\nsplit_by() creates a class object named split_df.\n\n\nlibrary(alookr)\n\n# split the data into a train set and a test set by default arguments\nsb <- breastCancer %>%\n  split_by(target = Class)\n\n# show the class name\nclass(sb)\n\n\n[1] \"split_df\"   \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n# split the data into a train set and a test set by ratio = 0.6\ntmp <- breastCancer %>%\n  split_by(Class, ratio = 0.6)\n\n\n\nThe summary() function displays the following useful information about the split_df object:\nrandom seed : The random seed is the random seed used internally to separate the data\nsplit data : Information of splited data\ntrain set count : number of train set\ntest set count : number of test set\n\ntarget variable : Target variable name\nminority class : name and ratio(In parentheses) of minority class\nmajority class : name and ratio(In parentheses) of majority class\n\n\n\n# summary() display the some information\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  90170 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\n\n# summary() display the some information\nsummary(tmp)\n\n\n** Split train/test set information **\n + random seed        :  85477 \n + split data            \n    - train set count :  419 \n    - test set count  :  280 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\nCheck missing levels in the train set\nIn the case of categorical variables, when a train set and a test set are separated, a specific level may be missing from the train set.\nIn this case, there is no problem when fitting the model, but an error occurs when predicting with the model you created. Therefore, preprocessing is performed to avoid missing data preprocessing.\nIn the following example, fortunately, there is no categorical variable that contains the missing levels in the train set.\n\n\n# list of categorical variables in the train set that contain missing levels\nnolevel_in_train <- sb %>%\n  compare_target_category() %>% \n  filter(is.na(train)) %>% \n  select(variable) %>% \n  unique() %>% \n  pull\n\nnolevel_in_train\n\n\ncharacter(0)\n\n\n# if any of the categorical variables in the train set contain a missing level, \n# split them again.\nwhile (length(nolevel_in_train) > 0) {\n  sb <- breastCancer %>%\n    split_by(Class)\n\n  nolevel_in_train <- sb %>%\n    compare_target_category() %>% \n    filter(is.na(train)) %>% \n    select(variable) %>% \n    unique() %>% \n    pull\n}\n\n\n\nHandling the imbalanced classes data with sampling_target()\nIssue of imbalanced classes data\nImbalanced classes(levels) data means that the number of one level of the frequency of the target variable is relatively small. In general, the proportion of positive classes is relatively small. For example, in the model of predicting spam, the class of interest spam is less than non-spam.\nImbalanced classes data is a common problem in machine learning classification.\ntable() and prop.table() are traditionally useful functions for diagnosing imbalanced classes data. However, alookr’s summary() is simpler and provides more information.\n\n\n# train set frequency table - imbalanced classes data\ntable(sb$Class)\n\n\n\n   benign malignant \n      458       241 \n\n\n# train set relative frequency table - imbalanced classes data\nprop.table(table(sb$Class))\n\n\n\n   benign malignant \n0.6552217 0.3447783 \n\n\n# using summary function - imbalanced classes data\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  90170 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\nHandling the imbalanced classes data\nMost machine learning algorithms work best when the number of samples in each class are about equal. And most algorithms are designed to maximize accuracy and reduce error. So, we requre handling an imbalanced class problem.\nsampling_target() performs sampling to solve an imbalanced classes data problem.\nResampling - oversample minority class\nOversampling can be defined as adding more copies of the minority class.\nOversampling is performed by specifying “ubOver” in the method argument of the sampling_target() function.\n\n\n# to balanced by over sampling\ntrain_over <- sb %>%\n  sampling_target(method = \"ubOver\")\n\n# frequency table \ntable(train_over$Class)\n\n\n\n   benign malignant \n      318       318 \n\nResampling - undersample majority class\nUndersampling can be defined as removing some observations of the majority class.\nUndersampling is performed by specifying “ubUnder” in the method argument of the sampling_target() function.\n\n\n# to balanced by under sampling\ntrain_under <- sb %>%\n  sampling_target(method = \"ubUnder\")\n\n# frequency table \ntable(train_under$Class)\n\n\n\n   benign malignant \n      171       171 \n\nGenerate synthetic samples - SMOTE\nSMOTE(Synthetic Minority Oversampling Technique) uses a nearest neighbors algorithm to generate new and synthetic data.\nSMOTE is performed by specifying “ubSMOTE” in the method argument of the sampling_target() function.\n\n\n# to balanced by SMOTE\ntrain_smote <- sb %>%\n  sampling_target(seed = 1234L, method = \"ubSMOTE\")\n\n# frequency table \ntable(train_smote$Class)\n\n\n\n   benign malignant \n      684       513 \n\nCleansing the dataset for classification modeling with cleanse()\nThe cleanse() cleanse the dataset for classification modeling.\nThis function is useful when fit the classification model. This function does the following.:\nRemove the variable with only one value.\nAnd remove variables that have a unique number of values relative to the number of observations for a character or categorical variable.\nIn this case, it is a variable that corresponds to an identifier or an identifier.\n\nAnd converts the character to factor.\nIn this example, The cleanse() function removed a variable ID with a high unique rate.\n\n\n# clean the training set\ntrain <- train_smote %>%\n  cleanse\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 424(0.35421888053467)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\nExtract test set for evaluation of the model with extract_set()\n\n\n# extract test set\ntest <- sb %>%\n  extract_set(set = \"test\")\n\n\n\nBinary classification modeling with run_models()\nrun_models() performs some representative binary classification modeling using split_df object created by split_by().\nrun_models() executes the process in parallel when fitting the model. However, it is not supported in MS-Windows operating system and RStudio environment.\nCurrently supported algorithms are as follows.:\nlogistic : logistic regression using stats package\nrpart : Recursive Partitioning Trees using rpart package\nctree : Conditional Inference Trees using party package\nrandomForest :Classification with Random Forest using randomForest package\nranger : A Fast Implementation of Random Forests using ranger package\nxgboost : Extreme Gradient Boosting using xgboost package\nrun_models() returns a model_df class object.\nThe model_df class object contains the following variables.:\nstep : character. The current stage in the classification modeling process.\nFor objects created with run_models(), the value of the variable is “1.Fitted”.\n\nmodel_id : model identifiers\ntarget : name of target variable\npositive : positive class in target variable\nfitted_model : list. Fitted model object by model_id’s algorithms\n\n\nresult <- train %>% \n  run_models(target = \"Class\", positive = \"malignant\")\nresult\n\n\n# A tibble: 7 x 7\n  step     model_id    target is_factor positive negative fitted_model\n  <chr>    <chr>       <chr>  <lgl>     <chr>    <chr>    <list>      \n1 1.Fitted logistic    Class  TRUE      maligna… benign   <glm>       \n2 1.Fitted rpart       Class  TRUE      maligna… benign   <rpart>     \n3 1.Fitted ctree       Class  TRUE      maligna… benign   <BinaryTr>  \n4 1.Fitted randomFore… Class  TRUE      maligna… benign   <rndmFrs.>  \n5 1.Fitted ranger      Class  TRUE      maligna… benign   <ranger>    \n6 1.Fitted xgboost     Class  TRUE      maligna… benign   <xgb.Bstr>  \n# … with 1 more row\n\nEvaluate the model\nEvaluate the predictive performance of fitted models.\nPredict test set using fitted model with run_predict()\nrun_predict() predict the test set using model_df class fitted by run_models().\nrun_predict () is executed in parallel when predicting by model. However, it is not supported in MS-Windows operating system and RStudio environment.\nThe model_df class object contains the following variables.:\nstep : character. The current stage in the classification modeling process.\nFor objects created with run_predict(), the value of the variable is “2.Predicted”.\n\nmodel_id : character. Type of fit model.\ntarget : character. Name of target variable.\npositive : character. Level of positive class of binary classification.\nfitted_model : list. Fitted model object by model_id’s algorithms.\npredicted : result of predcit by each models\n\n\npred <- result %>%\n  run_predict(test)\npred\n\n\n# A tibble: 7 x 8\n  step      model_id   target is_factor positive negative fitted_model\n  <chr>     <chr>      <chr>  <lgl>     <chr>    <chr>    <list>      \n1 2.Predic… logistic   Class  TRUE      maligna… benign   <glm>       \n2 2.Predic… rpart      Class  TRUE      maligna… benign   <rpart>     \n3 2.Predic… ctree      Class  TRUE      maligna… benign   <BinaryTr>  \n4 2.Predic… randomFor… Class  TRUE      maligna… benign   <rndmFrs.>  \n5 2.Predic… ranger     Class  TRUE      maligna… benign   <ranger>    \n6 2.Predic… xgboost    Class  TRUE      maligna… benign   <xgb.Bstr>  \n# … with 1 more row, and 1 more variable: predicted <list>\n\nCalculate the performance metric with run_performance()\nrun_performance() calculate the performance metric of model_df class predicted by run_predict().\nrun_performance () is performed in parallel when calculating the performance evaluation metrics However, it is not supported in MS-Windows operating system and RStudio environment.\nThe model_df class object contains the following variables.:\nstep : character. The current stage in the classification modeling process.\nFor objects created with run_performance(), the value of the variable is “3.Performanced”.\n\nmodel_id : character. Type of fit model.\ntarget : character. Name of target variable.\npositive : character. Level of positive class of binary classification.\nfitted_model : list. Fitted model object by model_id’s algorithms\npredicted : list. Predicted value by individual model. Each value has a predict_class class object.\nperformance : list. Calculate metrics by individual model. Each value has a numeric vector.\n\n\n# Calculate performace metrics.\nperf <- run_performance(pred)\nperf\n\n\n# A tibble: 7 x 7\n  step    model_id  target positive fitted_model predicted performance\n  <chr>   <chr>     <chr>  <chr>    <list>       <list>    <list>     \n1 3.Perf… logistic  Class  maligna… <glm>        <fct [21… <dbl [15]> \n2 3.Perf… rpart     Class  maligna… <rpart>      <fct [21… <dbl [15]> \n3 3.Perf… ctree     Class  maligna… <BinaryTr>   <fct [21… <dbl [15]> \n4 3.Perf… randomFo… Class  maligna… <rndmFrs.>   <fct [21… <dbl [15]> \n5 3.Perf… ranger    Class  maligna… <ranger>     <fct [21… <dbl [15]> \n6 3.Perf… xgboost   Class  maligna… <xgb.Bstr>   <fct [21… <dbl [15]> \n# … with 1 more row\n\nThe performance variable contains a list object, which contains 15 performance metrics:\nZeroOneLoss : Normalized Zero-One Loss(Classification Error Loss).\nAccuracy : Accuracy.\nPrecision : Precision.\nRecall : Recall.\nSensitivity : Sensitivity.\nSpecificity : Specificity.\nF1_Score : F1 Score.\nFbeta_Score : F-Beta Score.\nLogLoss : Log loss / Cross-Entropy Loss.\nAUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC).\nGini : Gini Coefficient.\nPRAUC : Area Under the Precision-Recall Curve (PR AUC).\nLiftAUC : Area Under the Lift Chart.\nGainAUC : Area Under the Gain Chart.\nKS_Stat : Kolmogorov-Smirnov Statistic.\n\n\n# Performance by analytics models\nperformance <- perf$performance\nnames(performance) <- perf$model_id\nperformance\n\n\n$logistic\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.06666667  0.93333333  0.90000000  0.90000000  0.90000000 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.95000000  0.90000000  0.90000000  2.23608577  0.93147959 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.85918367  0.06524816  1.23041695  0.78765306 86.42857143 \n\n$rpart\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.04761905  0.95238095  0.89473684  0.97142857  0.97142857 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.94285714  0.93150685  0.93150685  0.47177933  0.96418367 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.93836735  0.68264643  1.88355147  0.80945578 92.14285714 \n\n$ctree\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.05238095  0.94761905  0.88311688  0.97142857  0.97142857 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.93571429  0.92517007  0.92517007  0.41646571  0.98056122 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.96163265  0.79154066  1.99494847  0.82037415 91.42857143 \n\n$randomForest\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.01904762  0.98095238  0.94594595  1.00000000  1.00000000 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.97142857  0.97222222  0.97222222  0.07998188  0.99500000 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.99000000  0.88843776  2.03490054  0.83000000 97.14285714 \n\n$ranger\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.02857143  0.97142857  0.92105263  1.00000000  1.00000000 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.95714286  0.95890411  0.95890411  0.07413234  0.99673469 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.99346939  0.90723276  2.03167355  0.83115646 97.14285714 \n\n$xgboost\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.02857143  0.97142857  0.93243243  0.98571429  0.98571429 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.96428571  0.95833333  0.95833333  0.09591749  0.99489796 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.98959184  0.94665981  2.09969906  0.82993197 95.00000000 \n\n$lasso\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.02857143  0.97142857  0.95714286  0.95714286  0.95714286 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.97857143  0.95714286  0.95714286  0.05848341  0.99816327 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.99632653  0.98201545  2.08070669  0.83210884 97.14285714 \n\nIf you change the list object to tidy format, you’ll see the following at a glance:\n\n\n# Convert to matrix for compare performace.\nsapply(performance, \"c\")\n\n\n               logistic       rpart       ctree randomForest\nZeroOneLoss  0.06666667  0.04761905  0.05238095   0.01904762\nAccuracy     0.93333333  0.95238095  0.94761905   0.98095238\nPrecision    0.90000000  0.89473684  0.88311688   0.94594595\nRecall       0.90000000  0.97142857  0.97142857   1.00000000\nSensitivity  0.90000000  0.97142857  0.97142857   1.00000000\nSpecificity  0.95000000  0.94285714  0.93571429   0.97142857\nF1_Score     0.90000000  0.93150685  0.92517007   0.97222222\nFbeta_Score  0.90000000  0.93150685  0.92517007   0.97222222\nLogLoss      2.23608577  0.47177933  0.41646571   0.07998188\nAUC          0.93147959  0.96418367  0.98056122   0.99500000\nGini         0.85918367  0.93836735  0.96163265   0.99000000\nPRAUC        0.06524816  0.68264643  0.79154066   0.88843776\nLiftAUC      1.23041695  1.88355147  1.99494847   2.03490054\nGainAUC      0.78765306  0.80945578  0.82037415   0.83000000\nKS_Stat     86.42857143 92.14285714 91.42857143  97.14285714\n                 ranger     xgboost       lasso\nZeroOneLoss  0.02857143  0.02857143  0.02857143\nAccuracy     0.97142857  0.97142857  0.97142857\nPrecision    0.92105263  0.93243243  0.95714286\nRecall       1.00000000  0.98571429  0.95714286\nSensitivity  1.00000000  0.98571429  0.95714286\nSpecificity  0.95714286  0.96428571  0.97857143\nF1_Score     0.95890411  0.95833333  0.95714286\nFbeta_Score  0.95890411  0.95833333  0.95714286\nLogLoss      0.07413234  0.09591749  0.05848341\nAUC          0.99673469  0.99489796  0.99816327\nGini         0.99346939  0.98959184  0.99632653\nPRAUC        0.90723276  0.94665981  0.98201545\nLiftAUC      2.03167355  2.09969906  2.08070669\nGainAUC      0.83115646  0.82993197  0.83210884\nKS_Stat     97.14285714 95.00000000 97.14285714\n\ncompare_performance() return a list object(results of compared model performance). and list has the following components:\nrecommend_model : character. The name of the model that is recommended as the best among the various models.\ntop_count : numeric. The number of best performing performance metrics by model.\nmean_rank : numeric. Average of ranking individual performance metrics by model.\ntop_metric : list. The name of the performance metric with the best performance on individual performance metrics by model.\nIn this example, compare_performance() recommend the “ranger” model.\n\n\n# Compaire the Performance metrics of each model\ncomp_perf <- compare_performance(pred)\ncomp_perf\n\n\n$recommend_model\n[1] \"lasso\"\n\n$top_metric_count\n    logistic        rpart        ctree randomForest       ranger \n           0            0            0            4            2 \n     xgboost        lasso \n           1            7 \n\n$mean_rank\n    logistic        rpart        ctree randomForest       ranger \n    6.692308     5.576923     5.576923     2.307692     2.576923 \n     xgboost        lasso \n    3.153846     2.115385 \n\n$top_metric\n$top_metric$logistic\nNULL\n\n$top_metric$rpart\nNULL\n\n$top_metric$ctree\nNULL\n\n$top_metric$randomForest\n[1] \"ZeroOneLoss\" \"Accuracy\"    \"Recall\"      \"F1_Score\"   \n\n$top_metric$ranger\n[1] \"Recall\"  \"KS_Stat\"\n\n$top_metric$xgboost\n[1] \"LiftAUC\"\n\n$top_metric$lasso\n[1] \"Precision\"   \"Specificity\" \"LogLoss\"     \"AUC\"        \n[5] \"Gini\"        \"PRAUC\"       \"GainAUC\"    \n\nPlot the ROC curve with plot_performance()\ncompare_performance() plot ROC curve.\n\n\n# Plot ROC curve\nplot_performance(pred)\n\n\n\n\nTunning the cut-off\nIn general, if the prediction probability is greater than 0.5 in the binary classification model, it is predicted as positive class. In other words, 0.5 is used for the cut-off value. This applies to most model algorithms. However, in some cases, the performance can be tuned by changing the cut-off value.\nplot_cutoff () visualizes a plot to select the cut-off value, and returns the cut-off value.\n\n\npred_best <- pred %>% \n  filter(model_id == comp_perf$recommend_model) %>% \n  select(predicted) %>% \n  pull %>% \n  .[[1]] %>% \n  attr(\"pred_prob\")\n\ncutoff <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"mcc\")\n\n\n\ncutoff\n\n\n[1] 0.31\n\n\ncutoff2 <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"density\")\n\n\n\ncutoff2\n\n\n[1] 0.6908\n\n\ncutoff3 <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"prob\")\n\n\n\ncutoff3\n\n\n[1] 0.31\n\nPerformance comparison between prediction and tuned cut-off with performance_metric()\nCompare the performance of the original prediction with that of the tuned cut-off. Compare the cut-off with the non-cut model for the model with the best performance comp_perf$recommend_model.\n\n\ncomp_perf$recommend_model\n\n\n[1] \"lasso\"\n\n\n# extract predicted probability\nidx <- which(pred$model_id == comp_perf$recommend_model)\npred_prob <- attr(pred$predicted[[idx]], \"pred_prob\")\n\n# or, extract predicted probability using dplyr\npred_prob <- pred %>% \n  filter(model_id == comp_perf$recommend_model) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  attr(\"pred_prob\")\n\n# predicted probability\npred_prob  \n\n\n  [1] 0.0174789286 0.0519610507 0.0032789015 0.0175209919 0.0032869065\n  [6] 0.0064426070 0.0061333348 0.3549603767 0.9816825081 0.8597953337\n [11] 0.9586400416 0.9829636303 0.9807564163 0.8423412779 0.5789667959\n [16] 0.0018752648 0.0174789286 0.0198675394 0.9962768052 0.0343359964\n [21] 0.1091094158 0.9998443243 0.9999145915 0.0174789286 0.0036986645\n [26] 0.0032789015 0.9999991641 0.9963338820 0.0640467480 0.9998845141\n [31] 0.9999213746 0.0018752648 0.9996409596 0.0100335360 0.9295070011\n [36] 0.0062525812 0.0198675394 0.9981252268 0.0015038391 0.0018798495\n [41] 0.9855421605 0.0029222429 0.4612051835 0.2741038918 0.0026301961\n [46] 0.0989389839 0.0254864819 0.9881447326 0.9962328663 0.0057410803\n [51] 0.9932372955 0.0174789286 0.0519610507 0.9882845720 0.0046075035\n [56] 0.9112488848 0.0049576630 0.0174789286 0.0423490116 0.0010718527\n [61] 0.0057550617 0.0302801908 0.9999561536 0.9993322438 0.0032789015\n [66] 0.9999961972 0.9991843932 0.0036806781 0.9968751445 0.0026301961\n [71] 0.9799969986 0.6646602131 0.9880154141 0.0155439828 0.9987995118\n [76] 0.0015219772 0.9996213115 0.9990884566 0.9992499097 0.9980651483\n [81] 0.7459408405 0.0032789015 0.7459408405 0.0114572781 0.0065363079\n [86] 0.0032789015 0.9999947042 0.7308047117 0.0010718527 0.9906787589\n [91] 0.0032789015 0.0032789015 0.9349685763 0.0032789015 0.9994083226\n [96] 0.0040969162 0.0015038391 0.0063453940 0.8544840270 0.8291636536\n[101] 0.0032789015 0.0106386198 0.0594483184 0.0315084513 0.9794110022\n[106] 0.0032789015 0.9600304045 0.9946262997 0.9999201601 0.0010718527\n[111] 0.9999965110 0.0012181350 0.0017089763 0.9996232840 0.0057690770\n[116] 0.0087428342 0.0069747680 0.0142279160 0.1077781495 0.9992975276\n[121] 0.0018752648 0.0654855639 0.9774531461 0.0082488763 0.0013775228\n[126] 0.0018798495 0.9988106910 0.0101668011 0.0041516620 0.9976670706\n[131] 0.9996056910 0.0107418740 0.0089487465 0.0065917287 0.0141284785\n[136] 0.0032949311 0.0061482656 0.0057690770 0.9925235631 0.0080984931\n[141] 0.3111760831 0.3249935903 0.9841031438 0.0008594184 0.0076723952\n[146] 0.0100578652 0.0174789286 0.0058351459 0.0022745826 0.0010718527\n[151] 0.0015038391 0.0039423091 0.0013367271 0.0071863706 0.0174789286\n[156] 0.0223098968 0.0080788648 0.0057550617 0.0322182336 0.0100578652\n[161] 0.0303521088 0.9994192211 0.0015075171 0.9970521458 0.1240980381\n[166] 0.1239033127 0.0175209919 0.0302801908 0.0032789015 0.0057550617\n[171] 0.0106900042 0.0165141914 0.9975221618 0.0018752648 0.0186581120\n[176] 0.0107418740 0.9515212505 0.0100822529 0.9988956651 0.9999999199\n[181] 0.0026366216 0.0100578652 0.5456300156 0.0010718527 0.0051179763\n[186] 0.0263618680 0.0010718527 0.9999789834 0.0057550617 0.0015219772\n[191] 0.0013513340 0.0199857423 0.0057550617 0.0010718527 0.0018752648\n[196] 0.0021309597 0.9963922021 0.0057410803 0.0234847484 0.0018752648\n[201] 0.0100822529 0.0010718527 0.0010718527 0.0010718527 0.0057690770\n[206] 0.9286862826 0.0032949311 0.0115808996 0.9980659398 0.9854399625\n\n\n# compaire Accuracy\nperformance_metric(pred_prob, test$Class, \"malignant\", \"Accuracy\")\n\n\n[1] 0.9714286\n\nperformance_metric(pred_prob, test$Class, \"malignant\", \"Accuracy\",\n                   cutoff = cutoff)\n\n\n[1] 0.9809524\n\n\n# compaire Confusion Matrix\nperformance_metric(pred_prob, test$Class, \"malignant\", \"ConfusionMatrix\")\n\n\n           actual\npredict     benign malignant\n  benign       137         3\n  malignant      3        67\n\nperformance_metric(pred_prob, test$Class, \"malignant\", \"ConfusionMatrix\", \n                   cutoff = cutoff)\n\n\n           actual\npredict     benign malignant\n  benign       136         0\n  malignant      4        70\n\n\n# compaire F1 Score\nperformance_metric(pred_prob, test$Class, \"malignant\", \"F1_Score\")\n\n\n[1] 0.9571429\n\nperformance_metric(pred_prob, test$Class,  \"malignant\", \"F1_Score\", \n                   cutoff = cutoff)\n\n\n[1] 0.9722222\n\nperformance_metric(pred_prob, test$Class,  \"malignant\", \"F1_Score\", \n                   cutoff = cutoff2)\n\n\n[1] 0.9635036\n\nIf the performance of the tuned cut-off is good, use it as a cut-off to predict positives.\nPredict\nIf you have selected a good model from several models, then perform the prediction with that model.\nCreate data set for predict\nCreate sample data for predicting by extracting 100 samples from the data set used in the previous under sampling example.\n\n\ndata_pred <- train_under %>% \n  cleanse \n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 336(0.982456140350877)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\n\nset.seed(1234L)\ndata_pred <- data_pred %>% \n  nrow %>% \n  seq %>% \n  sample(size = 50) %>% \n  data_pred[., ]\n\n\n\nPredict with alookr and dplyr\nDo a predict using the dplyr package. The last factor() function eliminates unnecessary information.\n\n\npred_actual <- pred %>%\n  filter(model_id == comp_perf$recommend_model) %>% \n  run_predict(data_pred) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  factor()\n\npred_actual\n\n\n [1] benign    benign    benign    malignant malignant benign   \n [7] malignant benign    benign    benign    benign    benign   \n[13] benign    benign    malignant benign    malignant malignant\n[19] malignant benign    malignant benign    benign    malignant\n[25] benign    malignant malignant benign    benign    malignant\n[31] malignant malignant benign    malignant malignant benign   \n[37] malignant benign    malignant malignant malignant malignant\n[43] benign    benign    benign    malignant benign    benign   \n[49] malignant malignant\nLevels: benign malignant\n\nIf you want to predict by cut-off, specify the cutoff argument in the run_predict() function as follows.:\nIn the example, there is no difference between the results of using cut-off and not.\n\n\npred_actual2 <- pred %>%\n  filter(model_id == comp_perf$recommend_model) %>% \n  run_predict(data_pred, cutoff) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  factor()\n\npred_actual2\n\n\n [1] benign    benign    benign    malignant malignant benign   \n [7] malignant benign    benign    benign    benign    benign   \n[13] benign    benign    malignant benign    malignant malignant\n[19] malignant benign    malignant benign    benign    malignant\n[25] benign    malignant malignant benign    benign    malignant\n[31] malignant malignant benign    malignant malignant benign   \n[37] malignant benign    malignant malignant malignant malignant\n[43] benign    benign    benign    malignant benign    malignant\n[49] malignant malignant\nLevels: benign malignant\n\n\nsum(pred_actual != pred_actual2)\n\n\n[1] 1\n\n\n\n\n",
      "last_modified": "2021-11-30T08:28:09+09:00"
    },
    {
      "path": "split.html",
      "title": "Splitting the dataset",
      "description": "Split the data into a train set and a test set\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-30",
      "contents": "\n\nContents\nPreface\nData: Credit Card Default Data\nSplit dataset\nSplit dataset with split_by()\n\nCompare dataset\nComparison of categorical variables with compare_target_category()\nComparison of numeric variables with compare_target_numeric()\nComparison plot with compare_plot()\nDiagnosis of train set and test set with compare_diag()\n\nExtract train/test dataset\nExtract train set or test set with extract_set()\nExtract the data to fit the model with sampling_target()\n\n\n\n\n\nPreface\nTo develop a classification model, the original data must be divided into train data set and test data set. You should do the following:\nCleansing the dataset\nSplit the data into a train set and a test set\nSplit the data.frame or tbl_df into a train set and a test set\nCompare dataset\nComparison of categorical variables\nComparison of numeric variables\nDiagnosis of train set and test set\n\nExtract train/test dataset\nExtract train set or test set\nExtract the data to fit the model\n\n\nModeling and Evaluate, Predict\nThe alookr package makes these steps fast and easy:\nData: Credit Card Default Data\nDefault of ISLR package is a simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\nA data frame with 10000 observations on the following 4 variables.:\ndefault : factor. A factor with levels No and Yes indicating whether the customer defaulted on their debt\nstudent: factor. A factor with levels No and Yes indicating whether the customer is a student\nbalance: numeric. The average balance that the customer has remaining on their credit card after making their monthly payment\nincome : numeric. Income of customer\n\n\n# Credit Card Default Data\nhead(ISLR::Default)\n\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n\n# structure of dataset\nstr(ISLR::Default)\n\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\n\n# summary of dataset\nsummary(ISLR::Default)\n\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nSplit dataset\nsplit_by() splits the data.frame or tbl_df into a training set and a test set.\nSplit dataset with split_by()\nThe split_df class is created, which contains the split information and criteria to separate the training and the test set.\n\n\nlibrary(alookr)\nlibrary(dplyr)\n\n# Generate data for the example\nsb <- ISLR::Default %>%\n  split_by(default, seed = 6534)\n\nsb\n\n\n# A tibble: 10,000 x 5\n# Groups:   split_flag [2]\n  default student balance income split_flag\n  <fct>   <fct>     <dbl>  <dbl> <chr>     \n1 No      No         730. 44362. train     \n2 No      Yes        817. 12106. train     \n3 No      No        1074. 31767. train     \n4 No      No         529. 35704. train     \n5 No      No         786. 38463. test      \n6 No      Yes        920.  7492. train     \n# … with 9,994 more rows\n\nThe attributes of the split_df class are as follows.:\nsplit_seed : integer. random seed used for splitting\ntarget : character. the name of the target variable\nbinary : logical. whether the target variable is binary class\nminority : character. the name of the minority class\nmajority : character. the name of the majority class\nminority_rate : numeric. the rate of the minority class\nmajority_rate : numeric. the rate of the majority class\n\n\nattr_names <- names(attributes(sb))\nattr_names\n\n\n [1] \"names\"         \"row.names\"     \"groups\"        \"class\"        \n [5] \"split_seed\"    \"target\"        \"binary\"        \"minority\"     \n [9] \"majority\"      \"minority_rate\" \"majority_rate\"\n\n\nsb_attr <- attributes(sb)\n\n# The third property, row.names, is excluded from the output because its length is very long.\nsb_attr[!attr_names %in% \"row.names\"]\n\n\n$names\n[1] \"default\"    \"student\"    \"balance\"    \"income\"     \"split_flag\"\n\n$groups\n# A tibble: 2 x 2\n  split_flag       .rows\n  <chr>      <list<int>>\n1 test           [3,000]\n2 train          [7,000]\n\n$class\n[1] \"split_df\"   \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$split_seed\n[1] 6534\n\n$target\n  default \n\"default\" \n\n$binary\n[1] TRUE\n\n$minority\n[1] \"Yes\"\n\n$majority\n[1] \"No\"\n\n$minority_rate\n   Yes \n0.0333 \n\n$majority_rate\n    No \n0.9667 \n\nsummary() summarizes the information of two datasets splitted by split_by().\n\n\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  6534 \n + split data            \n    - train set count :  7000 \n    - test set count  :  3000 \n + target variable    :  default \n    - minority class  :  Yes (0.033300)\n    - majority class  :  No (0.966700)\n\nCompare dataset\nTrain data and test data should be similar. If the two datasets are not similar, the performance of the predictive model may be reduced.\nalookr provides a function to compare the similarity between train dataset and test dataset.\nIf the two data sets are not similar, the train dataset and test dataset should be splitted again from the original data.\nComparison of categorical variables with compare_target_category()\nCompare the statistics of the categorical variables of the train set and test set included in the “split_df” class.\n\n\nsb %>%\n  compare_target_category()\n\n\n# A tibble: 4 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 default  No    96.7  96.7   0.00476\n2 default  Yes    3.33  3.33  0.00476\n3 student  No    70.0  71.8   1.77   \n4 student  Yes   30.0  28.2   1.77   \n\n\n# compare variables that are character data types.\nsb %>%\n  compare_target_category(add_character = TRUE)\n\n\n# A tibble: 4 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 default  No    96.7  96.7   0.00476\n2 default  Yes    3.33  3.33  0.00476\n3 student  No    70.0  71.8   1.77   \n4 student  Yes   30.0  28.2   1.77   \n\n\n# display marginal\nsb %>%\n  compare_target_category(margin = TRUE)\n\n\n# A tibble: 6 x 5\n  variable level    train   test abs_diff\n  <chr>    <fct>    <dbl>  <dbl>    <dbl>\n1 default  No       96.7   96.7   0.00476\n2 default  Yes       3.33   3.33  0.00476\n3 default  <Total> 100    100     0.00952\n4 student  No       70.0   71.8   1.77   \n5 student  Yes      30.0   28.2   1.77   \n6 student  <Total> 100    100     3.54   \n\n\n# student variable only\nsb %>%\n  compare_target_category(student)\n\n\n# A tibble: 2 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 student  No     70.0  71.8     1.77\n2 student  Yes    30.0  28.2     1.77\n\n\nsb %>%\n  compare_target_category(student, margin = TRUE)\n\n\n# A tibble: 3 x 5\n  variable level   train  test abs_diff\n  <chr>    <fct>   <dbl> <dbl>    <dbl>\n1 student  No       70.0  71.8     1.77\n2 student  Yes      30.0  28.2     1.77\n3 student  <Total> 100   100       3.54\n\ncompare_target_category() returns tbl_df, where the variables have the following.:\nvariable : character. categorical variable name\nlevel : factor. level of categorical variables\ntrain : numeric. the relative frequency of the level in the train set\ntest : numeric. the relative frequency of the level in the test set\nabs_diff : numeric. the absolute value of the difference between two relative frequencies\nComparison of numeric variables with compare_target_numeric()\nCompare the statistics of the numerical variables of the train set and test set included in the “split_df” class.\n\n\nsb %>%\n  compare_target_numeric()\n\n\n# A tibble: 2 x 7\n  variable train_mean test_mean train_sd test_sd train_z test_z\n  <chr>         <dbl>     <dbl>    <dbl>   <dbl>   <dbl>  <dbl>\n1 balance        836.      834.     487.    477.    1.72   1.75\n2 income       33446.    33684.   13437.  13101.    2.49   2.57\n\n\n# balance variable only\nsb %>%\n  compare_target_numeric(balance)\n\n\n# A tibble: 1 x 7\n  variable train_mean test_mean train_sd test_sd train_z test_z\n  <chr>         <dbl>     <dbl>    <dbl>   <dbl>   <dbl>  <dbl>\n1 balance        836.      834.     487.    477.    1.72   1.75\n\ncompare_target_numeric() returns tbl_df, where the variables have the following.:\nvariable : character. numeric variable name\ntrain_mean : numeric. arithmetic mean of train set\ntest_mean : numeric. arithmetic mean of test set\ntrain_sd : numeric. standard deviation of train set\ntest_sd : numeric. standard deviation of test set\ntrain_z : numeric. the arithmetic mean of the train set divided by the standard deviation\ntest_z : numeric. the arithmetic mean of the test set divided by the standard deviation\nComparison plot with compare_plot()\nPlot compare information of the train set and test set included in the “split_df” class.\n\n\n# income variable only\nsb %>%\n  compare_plot(\"income\")\n\n\n\n\n# all varibales\nsb %>%\n  compare_plot()\n\n\n\n\nDiagnosis of train set and test set with compare_diag()\nDiagnosis of similarity between datasets splitted by train set and set included in the “split_df” class.\n\n\ndefaults <- ISLR::Default\ndefaults$id <- seq(NROW(defaults))\n\nset.seed(1)\ndefaults[sample(seq(NROW(defaults)), 3), \"student\"] <- NA\nset.seed(2)\ndefaults[sample(seq(NROW(defaults)), 10), \"balance\"] <- NA\n\nsb_2 <- defaults %>%\n  split_by(default)\n\nsb_2 %>%\n  compare_diag()\n\n\n$missing_value\n# A tibble: 2 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429        NA    \n2 balance                 5         0.0714         0.167\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 0 x 5\n# … with 5 variables: variables <chr>, train_uniqcount <int>,\n#   train_uniqrate <dbl>, test_uniqcount <int>, test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\n\nsb_2 %>%\n  compare_diag(add_character = TRUE)\n\n\n$missing_value\n# A tibble: 2 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429        NA    \n2 balance                 5         0.0714         0.167\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 0 x 5\n# … with 5 variables: variables <chr>, train_uniqcount <int>,\n#   train_uniqrate <dbl>, test_uniqcount <int>, test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\n\nsb_2 %>%\n  compare_diag(uniq_thres = 0.0005)\n\n\n$missing_value\n# A tibble: 2 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429        NA    \n2 balance                 5         0.0714         0.167\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 2 x 5\n  variables train_uniqcount train_uniqrate test_uniqcount\n  <chr>               <int>          <dbl>          <int>\n1 default                NA             NA              2\n2 student                NA             NA              2\n# … with 1 more variable: test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\nExtract train/test dataset\nIf you compare the train set with the test set and find that the two datasets are similar, extract the data from the split_df object.\nExtract train set or test set with extract_set()\nExtract train set or test set from split_df class object.\n\n\ntrain <- sb %>%\n  extract_set(set = \"train\")\n\ntest <- sb %>%\n  extract_set(set = \"test\")\n\ndim(train)\n\n\n[1] 7000    4\n\n\ndim(test)\n\n\n[1] 3000    4\n\nExtract the data to fit the model with sampling_target()\nIn a target class, the ratio of the majority class to the minority class is not similar and the ratio of the minority class is very small, which is called the imbalanced class.\nIf target variable is an imbalanced class, the characteristics of the majority class are actively reflected in the model. This model implies an error in predicting the minority class as the majority class. So we have to make the train dataset a balanced class.\nsampling_target() performs sampling on the train set of split_df to resolve the imbalanced class.\n\n\n# under-sampling with random seed\nunder <- sb %>%\n  sampling_target(seed = 1234L)\n\nunder %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        233\n2 Yes       233\n\n\n# under-sampling with random seed, and minority class frequency is 40%\nunder40 <- sb %>%\n  sampling_target(seed = 1234L, perc = 40)\n\nunder40 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        349\n2 Yes       233\n\n\n# over-sampling with random seed\nover <- sb %>%\n  sampling_target(method = \"ubOver\", seed = 1234L)\n\nover %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       6767\n2 Yes      6767\n\n\n# over-sampling with random seed, and k = 10\nover10 <- sb %>%\n  sampling_target(method = \"ubOver\", seed = 1234L, k = 10)\n\nover10 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       6767\n2 Yes      2330\n\n\n# SMOTE with random seed\nsmote <- sb %>%\n  sampling_target(method = \"ubSMOTE\", seed = 1234L)\n\nsmote %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        932\n2 Yes       699\n\n\n# SMOTE with random seed, and perc.under = 250\nsmote250 <- sb %>%\n  sampling_target(method = \"ubSMOTE\", seed = 1234L, perc.under = 250)\n\nsmote250 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       1165\n2 Yes       699\n\nThe argument that specifies the sampling method in sampling_target () is method. “ubUnder” is under-sampling, and “ubOver” is over-sampling, “ubSMOTE” is SMOTE(Synthetic Minority Over-sampling TEchnique).\n\n\n\n",
      "last_modified": "2021-11-30T08:28:14+09:00"
    }
  ],
  "collections": []
}
